{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_mining_basics.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOZQNUGbjE66E5HC83AGh5p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-tomas/transform4europe/blob/main/notebooks/text_mining_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1AwPWOa4LTA"
      },
      "source": [
        "# Text mining basics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will work different text mining tasks: part-of-speech tagging, parsing and semantic analysis."
      ],
      "metadata": {
        "id": "8RGu0BMP_659"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vftAQFLq5Qa8"
      },
      "source": [
        "## Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSoUiL_W4Eg5"
      },
      "source": [
        "# Import the required libraries\n",
        "\n",
        "import spacy  # NLP library\n",
        "import pandas as pd  # Table manipulation\n",
        "import matplotlib.pyplot as plt  # Visualisation\n",
        "import seaborn as sns  # Visualisation\n",
        "import nltk  # NLP library\n",
        "from nltk.corpus import wordnet  # WordNet\n",
        "\n",
        "# Install the SpaCy model for English texts\n",
        "spacy.cli.download('en_core_web_sm')\n",
        "\n",
        "# Download WordNet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Download example text file ('news.txt')\n",
        "!wget https://raw.githubusercontent.com/d-tomas/transform4europe/main/datasets/news.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_YOZet95b0m"
      },
      "source": [
        "## Part-of-speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of *part-of-speech* (POS) *tagging* is to assing to each word in a text a particular part of speech, i.e., to identify whether they are nouns, verbs, adjectives, adverbs, etc."
      ],
      "metadata": {
        "id": "BHRJ1NTVBN5K"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOzyeno7fddD"
      },
      "source": [
        "# Process and annotate text with the SpaCy model\n",
        "\n",
        "text = 'Today is Monday, May 23, 2022. It is 6:00 p.m. I am attending a Text Mining seminar at the University of Alicante, in Spain. The teacher is David. He tries to make it interesting but sometimes fails.'\n",
        "document = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXSI__07fmZz"
      },
      "source": [
        "# Extract the list of sentences from text\n",
        "\n",
        "list(document.sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abl7bLud5hK2"
      },
      "source": [
        "# Extract morphological information (POS-tagging) for each word in text\n",
        "\n",
        "for token in document:  # For each token (word) in the document\n",
        "    print('Word: ' + token.text)\n",
        "    print('Lemma: ' + token.lemma_)\n",
        "    print('POS: ' + token.pos_)\n",
        "    print('POS fine: ' + token.tag_)\n",
        "    print('---')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfPvMReXedYw"
      },
      "source": [
        "# You can use 'explain' if you do not understand the meaning of a POS tag\n",
        "\n",
        "spacy.explain('CD')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COVPXVTnV5YF"
      },
      "source": [
        "# Create a Pandas DataFrame based on the content for further analysis\n",
        "\n",
        "data = pd.DataFrame(data=[[token.text, token.lemma_, token.pos_, token.tag_] for token in document], columns=['Word', 'Lemma', 'POS', 'POS fine'])\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ZAGLDzY2im"
      },
      "source": [
        "# Basic statistics of the columns\n",
        "\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFGlCZXkY9Pn"
      },
      "source": [
        "# What is the number of verbs in the text?\n",
        "\n",
        "(data['POS'] == 'VERB').sum()  # Substitute 'VERB' with any other POS tag (e.g. 'PUNCT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxHivGQjX516"
      },
      "source": [
        "# we can do some interesting visualisations\n",
        "# Bar plot with the count of each POS (fine) tag\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "sns.countplot(x='POS fine', data=data, order=data['POS fine'].value_counts().index)  # Sort by frequency\n",
        "plt.xticks(rotation=-45)  # Rotate the labels to avoid overlapping\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7cC9UAb_-bs"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFbY0N1xaDZA"
      },
      "source": [
        "Do the POS-tagging of the content in the file 'news.txt'. How many adjectives are there in the text?\n",
        "\n",
        "**Tip**: load the information in Pandas DataFrame to manipulate it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocChesdh_7s2"
      },
      "source": [
        "# First we have to store all the content of 'news.txt' in the variable 'content'\n",
        "with open('news.txt') as file:\n",
        "    text = file.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n"
      ],
      "metadata": {
        "id": "RfFvGGxoD8Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQt_nNnfDZLi"
      },
      "source": [
        "## Shallow parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFwX-dlrDbQd"
      },
      "source": [
        "# Get all the noun phrases from text\n",
        "\n",
        "for chunk in document.noun_chunks:\n",
        "    print('Noun phrase: ' + chunk.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uCmyakyD4qo"
      },
      "source": [
        "# 'displacy' shows the parse tree\n",
        "\n",
        "spacy.displacy.render(document, style = 'dep', options = {'compact': True}, jupyter = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMCnuMABD7xI"
      },
      "source": [
        "# Navigate the dependency tree\n",
        "# - 'head' and 'child' describe words connected in the dependency tree\n",
        "# - 'dep' is the type of syntactic relation connecting 'child' and 'head'\n",
        "\n",
        "for token in document:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_, [child for child in token.children])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpPBskWToUbJ"
      },
      "source": [
        "# A very nice feature: Named Entity Recognition\n",
        "# Full list of entity types recognised: https://spacy.io/api/annotation#named-entities\n",
        "\n",
        "for ent in document.ents:\n",
        "    print('Text: ' + ent.text)\n",
        "    print('Start char: ' + str(ent.start_char))\n",
        "    print('End char: ' + str(ent.end_char))\n",
        "    print('Type: ' + ent.label_)\n",
        "    print('---')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPA14c3asYx9"
      },
      "source": [
        "# Highlight named entities and their labels in a text\n",
        "\n",
        "spacy.displacy.render(document, style='ent', options = {'distance': 90}, jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo-lGalAUgZN"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skl81t_Rup2Z"
      },
      "source": [
        "Complete the code in the following cell to show the (shallow) parse tree of the content in the file 'news.txt'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGsn0SXwUko2"
      },
      "source": [
        "# Your code goes here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeTHC7mJuupi"
      },
      "source": [
        "Highlight the named entities in the file 'news.txt' using 'displacy'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6Ofvle8ukgb"
      },
      "source": [
        "# Your code goes here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEQaa0E6_smk"
      },
      "source": [
        "## Semantic analysis with WordNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8-b-4sxAwKF"
      },
      "source": [
        "# Get all the synsets of a word\n",
        "\n",
        "word = 'dog'\n",
        "\n",
        "list_synsets = wordnet.synsets(word)\n",
        "for synset in list_synsets:\n",
        "    print('Synset: ' + synset.name())\n",
        "    print('Lemma: ' + synset.lemmas()[0].name())\n",
        "    print('Meaning: ' + synset.definition())\n",
        "    print('Examples: ' + str(synset.examples()))\n",
        "    print('---')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwUnezNoCt1l"
      },
      "source": [
        "# Get synonyms and antonyms\n",
        "\n",
        "word = 'tall'\n",
        "\n",
        "list_synsets = wordnet.synsets(word)\n",
        "list_sinonyms = set()  # Use 'set' instead of 'list' to avoid duplicates\n",
        "list_antonyms = set()\n",
        "for synset in list_synsets:\n",
        "    for lemma in synset.lemmas():\n",
        "        list_sinonyms.add(lemma.name())\n",
        "        if lemma.antonyms():\n",
        "            list_antonyms.add(lemma.antonyms()[0].name())\n",
        "\n",
        "print('Synonyms: ' + str(list_sinonyms))\n",
        "print('Antonyms: ' + str(list_antonyms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7GT83mZacX3"
      },
      "source": [
        "# Get all the hypernyms\n",
        "\n",
        "word = 'terrier'\n",
        "\n",
        "synset = wordnet.synsets(word)[0]  # First synset of the word\n",
        "hypernyms = lambda s:s.hypernyms()\n",
        "\n",
        "print(list(synset.closure(hypernyms)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}