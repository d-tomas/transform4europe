{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMbxr0iN96eX8vLAFN0M4wG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-tomas/transform4europe/blob/main/notebooks/document_representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1AwPWOa4LTA"
      },
      "source": [
        "# Document representation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this *notebook* we will review different techniques to transform textual representations into numerical vectors, such as TF-IDF weighting schema and word embeddings."
      ],
      "metadata": {
        "id": "qNAp-cOOGSaY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vftAQFLq5Qa8"
      },
      "source": [
        "## Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Transformers library\n",
        "\n",
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "lvaZ7XxEXAbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSoUiL_W4Eg5"
      },
      "source": [
        "# Import the required libraries\n",
        "\n",
        "import gensim  # Word embedding models\n",
        "import gensim.downloader  # Download pre-trained word embedding models\n",
        "from gensim.models import KeyedVectors  # Load pre-trained word embedding models\n",
        "import matplotlib.pyplot as plt  # Display word clouds\n",
        "import nltk  # NLP library\n",
        "from nltk.stem.porter import *  # Stemmer tool\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Term by document matrix with TF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Term by document matrix with TF-IDF\n",
        "import spacy  # NLP library\n",
        "from transformers import pipeline  # Transformer models\n",
        "\n",
        "# Install the SpaCy model for English texts\n",
        "spacy.cli.download('en_core_web_sm')\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Download example text files ('news.txt' and 'alices_adventures_in_wonderand.txt')\n",
        "!wget https://raw.githubusercontent.com/d-tomas/transform4europe/main/datasets/news.txt\n",
        "!wget https://raw.githubusercontent.com/d-tomas/transform4europe/main/datasets/alices_adventures_in_wonderland.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmCPGSE1RjBj"
      },
      "source": [
        "## N-gram extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTe_q_4VUZyk"
      },
      "source": [
        "# Extract bigrams and trigrams from text\n",
        "\n",
        "with open('news.txt') as file:\n",
        "    content = file.read()\n",
        "\n",
        "list_bigrams = nltk.ngrams(content.split(), 2)  # split() the sentence into a list of words\n",
        "list_trigrams = nltk.ngrams(content.split(), 3)\n",
        "\n",
        "print('---------')\n",
        "print('Bigrams:')\n",
        "print('---------')\n",
        "for bigram in list_bigrams:\n",
        "  print(bigram)\n",
        "\n",
        "print('----------')\n",
        "print('Trigrams:')\n",
        "print('----------')\n",
        "for trigram in list_trigrams:\n",
        "  print(trigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_NRVKsGSQb6"
      },
      "source": [
        "# The previous approach does not consider sentence boundaries\n",
        "# We can read the file line by line and extract n-grams for each line separately\n",
        "\n",
        "with open('news.txt') as file:\n",
        "    content = file.readlines()  # Get a list of lines\n",
        "\n",
        "# Remove empty lines, blanks and new line characters\n",
        "content = [line.strip() for line in content if line.strip()]\n",
        "\n",
        "for line in content:\n",
        "    trigrams = nltk.ngrams(line.split(), 3)  # Extract 3-grams for each line\n",
        "    for trigram in trigrams:\n",
        "        print(trigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxkaGPOSVtk6"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoxeREt_VYjd"
      },
      "source": [
        "# Repeat the analysis on 'alices_adventures_in_wonderland.txt', obtaining also 4-grams and 5-grams in addition to bigrams and trigrams\n",
        "# Use the first procedure (no need to consider sentence boundaries)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aDi5mm9aj3R"
      },
      "source": [
        "## Normalisation / pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzG05ZncXWcK"
      },
      "source": [
        "# Remove punctuation, lowercase, remove stopwords and get the stem of the words\n",
        "\n",
        "text = 'The Netherlands earned sweet revenge on Spain on Friday at the Fonte Nova in Salvador, hammering Spain 5-1 to put an emphatic coda on their loss in the 2010 World Cup finals.'\n",
        "\n",
        "document = nlp(text)  # Process the text with SpaCy\n",
        "\n",
        "document = [token for token in document if not token.is_punct]  # Remove punctuation\n",
        "print('No punctuation: ' + str(document))\n",
        "\n",
        "document = [token for token in document if not token.is_stop]  # Remove stopwords\n",
        "print('No stopwords: ' + str(document))\n",
        "\n",
        "document = [token.lower_ for token in document]  # Lowercase\n",
        "print('Lowercased: ' + str(document))\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "document = [stemmer.stem(token) for token in document]  # Stem of the words\n",
        "print('Stems: ' + str(document))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5gASp9Me70T"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqDpnBNoe7Ki"
      },
      "source": [
        "# Repeat the previous analysis on the content of 'alices_adventures_in_wonderland.txt'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmOJ7nl1dwCi"
      },
      "source": [
        "## Weighting schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5bTyix3dyf2"
      },
      "source": [
        "# Build the term by document matrix using the TF weighting schema\n",
        "\n",
        "corpus = ['I do not like this restaurant', 'I like this restaurant very much', 'I think it is a very very bad place', 'I love this place']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())\n",
        "print(X.shape)\n",
        "\n",
        "vectorizer2 = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))  # Extract bigrams\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "print(vectorizer2.get_feature_names_out())\n",
        "print(X2.toarray())\n",
        "print(X2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uORvGBOGet-j"
      },
      "source": [
        "# Build the term by document matrix using the TF-IDF weighting schema\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xbsFpsjhr4l"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FRsB7FFhtOs"
      },
      "source": [
        "# Get the term by document matrix, using TF weighting schema and trigrams on 'news.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMubljgLGzPm"
      },
      "source": [
        "## Word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and load into memory a word embedding model pre-trained with 100 billion words from Google News\n",
        "# It's gonna take a while...\n",
        "\n",
        "model = gensim.downloader.load('glove-wiki-gigaword-300')"
      ],
      "metadata": {
        "id": "tFwrFv3XMgEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A08x0V62MyAB"
      },
      "source": [
        "# Show the vector representing a word\n",
        "\n",
        "model['dog']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8QwhlbDfq0G"
      },
      "source": [
        "# Check the size of the returned vector\n",
        "\n",
        "len(model['dog'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpfnvoUQM0qx"
      },
      "source": [
        "# Get the 5 most similar words to a given one \n",
        "\n",
        "model.most_similar('desert', topn = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8LF3h5bAr7V"
      },
      "source": [
        "# Analogy: 'France' is to 'Paris' as 'Madrid' is to... (France - Paris + Madrid)\n",
        "# The model is lowercased, thus we cannot use capitalised tokens\n",
        "\n",
        "model.most_similar(positive=['madrid', 'france'], negative=['paris'], topn=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg2WJxX8AtwY"
      },
      "source": [
        "# Ditch unrelated terms\n",
        "\n",
        "model.doesnt_match(['wine', 'beer', 'coke', 'whysky'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc5NtJaaAvK5"
      },
      "source": [
        "# Similarity between words\n",
        "# Beware of algorithmic bias!!\n",
        "\n",
        "model.similarity('woman', 'housework')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers"
      ],
      "metadata": {
        "id": "wFcmT480JJhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🤗 [Transformers](https://huggingface.co/transformers/) library provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with deep interoperability between Jax, PyTorch and TensorFlow.\n",
        "\n",
        "There are more than 30,000 pre-trained [models](https://huggingface.co/models) and 2,000 [datasets](https://huggingface.co/datasets) available in their web page, covering tenths of different tasks in more than 100 languages.\n",
        "\n",
        "This demo exemplifies the use of [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html). These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, and Question Answering.\n",
        "\n",
        "The following examples are inspired in the 🤗 Transformers library [course](https://huggingface.co/course/chapter1/3?fw=pt)."
      ],
      "metadata": {
        "id": "yepfQOfsT2TV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment analysis\n",
        "Classify a sentence according to positive or negative sentiments."
      ],
      "metadata": {
        "id": "NP0h6SH9UOyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the sentiment analysis model ('distilbert-base-uncased-finetuned-sst-2-english' by default)\n",
        "\n",
        "model = pipeline('sentiment-analysis')"
      ],
      "metadata": {
        "id": "1qYTsSmsUOeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it!\n",
        "\n",
        "model('This is the best course I have ever attended in my life. Praise to David!')"
      ],
      "metadata": {
        "id": "EEM3QIZsJNfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot classification\n",
        "Classify text according to a set of given labels."
      ],
      "metadata": {
        "id": "8E2X6c4dUgU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the zero-shot classification model ('facebook/bart-large-mnli' by default)\n",
        "\n",
        "model = pipeline('zero-shot-classification')"
      ],
      "metadata": {
        "id": "70SLL3TEUhcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it!\n",
        "\n",
        "model('This lecture is about Natural Language Processing', candidate_labels=['education', 'politics', 'business', 'sports'])"
      ],
      "metadata": {
        "id": "FDrw1-_yUpWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text generation\n",
        "Predict the words that will follow a specified text prompt, creating a coherent portion of text that is a continuation from the given context."
      ],
      "metadata": {
        "id": "-P1XSZNNUrtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the text generation model ('gpt2' by default)\n",
        "\n",
        "model = pipeline('text-generation')"
      ],
      "metadata": {
        "id": "oVVb8s90UuzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it! (you will get a different output each time)\n",
        "\n",
        "model('I opened the door and found')"
      ],
      "metadata": {
        "id": "o01rn9HuUyKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tyr it tuning some parameters (maximum length generated and number of returned sentences)!\n",
        "\n",
        "model('The book was amazing', max_length=40, num_return_sequences=3)"
      ],
      "metadata": {
        "id": "uzId3nRmU3gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masked language modelling\n",
        "Mask a token in a sequence with a masking token, and prompt the model to fill that mask with an appropriate token."
      ],
      "metadata": {
        "id": "qRHUcQhpU64z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the masked language modelling model ('distilroberta-base' by default)\n",
        "\n",
        "model = pipeline('fill-mask')"
      ],
      "metadata": {
        "id": "jh6iffScU9Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it (returning the 'top_k' words)!\n",
        "\n",
        "model('I <mask> this lecture.', top_k=5)"
      ],
      "metadata": {
        "id": "IRBbJwgQVAkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named entity recognition\n",
        "Classify tokens according to a class (e.g. person, organisation or location)."
      ],
      "metadata": {
        "id": "Da3YNq1fVMJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the named entity recognition model ('dbmdz/bert-large-cased-finetuned-conll03-english' by default)\n",
        "\n",
        "model = pipeline('ner', grouped_entities=True)"
      ],
      "metadata": {
        "id": "337JIuI6VNWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it!\n",
        "\n",
        "model('My name is David and I live in Spain.')"
      ],
      "metadata": {
        "id": "FaafRIXFVT4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question answering\n",
        "Extract an answer from a text given a question."
      ],
      "metadata": {
        "id": "W1HwbayhVXhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the question answering model ('distilbert-base-cased-distilled-squad' by default)\n",
        "\n",
        "model = pipeline('question-answering')"
      ],
      "metadata": {
        "id": "-kbGXwWdVbI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it!\n",
        "\n",
        "model(question='Where do I work?', context='My name is David and I work really hard at the Unviersity of Alicante')"
      ],
      "metadata": {
        "id": "heLF-W0-VghZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine translation\n",
        "Translate from one language to another."
      ],
      "metadata": {
        "id": "_ypDAnRBVk2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the machine translation model from ES to EN ('Helsinki-NLP/opus-mt-es-en')\n",
        "# Try different models changing 'Helsinki-NLP/opus-mt-{src}-{tgt}' (src = source language, tgt = target)\n",
        "\n",
        "model = pipeline('translation', model='Helsinki-NLP/opus-mt-es-en')"
      ],
      "metadata": {
        "id": "CQpd7o0GVphq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it!\n",
        "\n",
        "model('Ojalá el próximo año pueda ir a Alicante')"
      ],
      "metadata": {
        "id": "LsgKbvJZVrkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuGoggVzTv4_"
      },
      "source": [
        "# References\n",
        "\n",
        "* [Alice's adventures in Wonderland](https://www.gutenberg.org/ebooks/11)\n",
        "* [Gensim](https://radimrehurek.com/gensim/index.html)\n",
        "* [Hugging Face](https://huggingface.co/)"
      ]
    }
  ]
}